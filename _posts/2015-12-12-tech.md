---
layout: post
title: Big-O Notation & Complexity
categories:
- blog
---

Well I had to go and pick the this confusing one…but here goes.

What is Big O? In typical encounters, you’ll see Big O used in notations like this:

O(1), O(n), O(n<sup>2</sup>), O(log(n))

Before I explain what this is supposed to mean, I want to first review some high school math. Remember equations?

#####y = x
![linear](/imgs/techblog/linear.png "Linear")

#####y = 1
![constant](/imgs/techblog/constant.png "Constant")

#####y = x<sup>2</sup>
![quadratic](/imgs/techblog/quadratic.png "Quadratic")

#####y = log(x)
![logarithmic](/imgs/techblog/logarithmic.png "Logarithmic")

Remember what they mean?

Basically, as a quick refresher, the line in the graph reflects change of y as x increases in value from left to right. So in the `y = x` graph, you’ll see that there’s a straight line, because y increases at the same rate that x does.

Now, I want you to look back at the Big O connotations I use, and look at them the same way. In fact, someone has put together a chart with various Big-O graphs, and look, you’ll see it’s no different from the charts I drew above.

![big-o](/imgs/techblog/big-o.png "Big-O Graph")
   _(Credits to <a href="http://www.bigocheatsheet.com">bigocheatsheet.com</a>)_


Now, that you know what they look like, we can start talking about what the values mean.

The O in Big-O stands for “Operations”, as in the number of operations that needs to be executed. The x-axis measures the number of users/arguments used in a function, while the y-axis measures the computational intensity (Operations). Essentially, Big-O is an indicator of how time-intensive an algorithm is (basically any method/function within a program) as it scales.

Algorithms take parameters, and sometimes a lot of them, and Big-O is the ultimate calculation that tells you how intensive this algorithm will become as we increase the number of arguments by large orders of magnitude (so we’re not comparing incrementally between 1 and 2, but rather, 1 to 10, and then 10 to 100, and then 100 to 1000, etc...)

Let’s consider an example:

{% highlight ruby %}
 def linear(array)
   array.each do |x|
     puts x
   end
 end

 linear([1]) # => “1”
 linear([1,2]) # => “1” “2”
 linear([1,2,3]) # => “1” “2” “3”
{% endhighlight %}

Notice if we only include one element in the array, then this function will run one calculation. If we have two, it will run two lines of calculation. This function’s complexity(this is actual terminology) is hence O(n) (much like the line `y=x`), because it’s a linear relationship between the number of elements and the time it takes to run.

Now let’s look at a different example:

{% highlight ruby %}
 def quadratic(array)
   array.each do |x|
     array.each do|y|
       puts "#{x}, #{y}"
     end
   end
 end
 quadratic([1]) # => “1,1” (1 calculation)

 quadratic([1,2]) # => “1,1” “1,2” “2,1” “2,2” (4 calculation)
 quadratic([1,2,3]) # => “1,1” “1,2” “1,3” “2,1” “2,2” “2,3” “3,1” “3,2” “3,3” (9 calculation)
{% endhighlight %}

In this code you’ll find that the calculation effort is scaled exponentially with more arguments. When we have 3 parameters the code will take 9 times as long, and with 4 parameters, 16 times as long. Not very ideal, but it can happen. In this case, the complexity of this code id O(n<sup>2</sup>). Get the gist?

O(1) will always be the most favored condition, because it runs consistently regardless of the number of inputs.

Here’s something I am copying from a stackoverflow answer, since there shouldn’t be a need to reinvent the wheel:

> O(n<sup>2</sup>): known as Quadratic complexity
>
> * 1 item: 1 second
> * 10 items: 100 seconds
> * 100 items: 10000 seconds
>
> Notice that the number of items increases by a factor of 10, but the time increases by a factor of 102. Basically, n=10 and so O(n^2) gives us the scaling factor n2 which is 102.
> O(n): known as Linear complexity
>
> * 1 item: 1 second
> * 10 items: 10 seconds
> * 100 items: 100 seconds
>
> This time the number of items increases by a factor of 10, and so does the time. n=10 and so O(n)'s scaling factor is 10.
> O(1): known as Constant complexity
>
> * 1 item: 1 second
> * 10 items: 1 second
> * 100 items: 1 second
>
> The number of items is still increasing by a factor of 10, but the scaling factor of O(1) is always 1.
> O(log n): known as Logarithmic complexity
>
> * 1 item: 1 second
> * 10 items: 2 seconds
> * 100 items: 3 seconds
> * 1000 items: 4 seconds
> * 10000 items: 5 seconds

   _<a href="http://stackoverflow.com/a/487300">- Credits to Ray Hidayat</a>_


So what’s the relevance of all this complicated, abstract science? Well when it comes to coding, a lot of times a code can work fine and dandy during test/beta stages when you’re working with small sample sizes, whether its the number of users or sets of data points. However, I think most programmers code with the intent of eventually having thousands or millions of users. In this case, when we’re talking about O(n<sup>2</sup>) or more intensive complexities, programs and servers can collapse when datasets increase by orders of magnitude. A 10-fold increase in the numbers of users could result in 100 fold increase in computer power, so if these types of scaling issues aren’t carefully accounted for, a lot of issues could arise.

Big-O can get very complicated, as programmers try to optimize their code for best efficiency, while maintaining readability of the code, and accounting for how big the datasets are going to get. For example, there are cases where codes are less efficient when there are a few arguments, but as n gets larger, the benefits become more obvious. That’s when it’s important to conduct a cost-benefit analysis, to determine which code will work best in the long run by accounting for long term goals.

This is just the tip of the iceberg, but I recommend you check out the following links for some great explanations.

Good luck in your learning, and happy coding!

####Further reading:

* <a href="http://stackoverflow.com/questions/487258/plain-english-explanation-of-big-o">http://stackoverflow.com/questions/487258/plain-english-explanation-of-big-o</a>
* <a href="https://justin.abrah.ms/computer-science/big-o-notation-explained.html">https://justin.abrah.ms/computer-science/big-o-notation-explained.html</a>
* <a href="http://bigocheatsheet.com/">http://bigocheatsheet.com/</a>
