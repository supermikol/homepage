<!DOCTYPE html>
<html>
  <head>
    <link rel="stylesheet" type="text/css" href="../stylesheets/default.css">
    <link rel="stylesheet" type="text/css" href="../stylesheets/blog.css">
    <title>Big-O Complexity and Notation</title>
    <meta charset="UTF-8">
  </head>

  <body>
    <header>
      <div id="tagline">
        <h4>Saving the world, one debug at a time</h4>
      </div>
      <nav>
        <ul>
          <li><a href="../projects.html">PROJECTS</a></li>
          <li><a href="../about.html">ABOUT</a></li>
          <li><a href="index.html">BLOG</a></li>
          <li><a href="../resume.html">RESUME</a></li>
          <li><a href="../contact.html">CONTACT</a></li>
        </ul>
      </nav>
    </header>

    <div id="right_bar">
      <h1><a href="../index.html">@supermikol</a></h1>
      <div id="profile_pic">
        <img src="../imgs/profilepic.jpg" alt="no photo">
      </div>
      <div id="socialmedia_links">
        <ul>
          <li id="li_1">Find me on:</li>
          <li><a href="https://cn.linkedin.com/in/michael-du-4927555"><img src="../imgs/linkedin_logo.png" alt="linkedin logo">LinkedIn</a></li>
          <li><a href="http://supermikol.github.io"><img src="../imgs/github_logo.png" alt="github logo">GitHub</a></li>
          <li><a href="https://www.quora.com/profile/Michael-Du-5"><img src="../imgs/quora_logo.png" alt="quora logo">Quora</a></li>
          <li><a href="https://www.facebook.com/mikedu"><img src="../imgs/facebook_logo.png" alt="facebook logo">Facebook</a></li>
          <li><a href="http://www.zhihu.com/people/michael-du-56"><img src="../imgs/zhihu_logo.png" alt="zhihu logo">Zhihu</a></li>
        </ul>
      </div>
      </div>

      <article class="blog-content">
        <h2>Big-O Notation & Complexity</h2>
        <div class="date">12/12/2015</div>
          <p>Well I had to go and pick the this confusing one…but here goes.</p>

          <p>What is Big O? In typical encounters, you’ll see Big O used in notations like this:</p>

          <div class="equation">O(1), O(n), O(n2), O(log(n))</div>

          <p>Before I explain what this is supposed to mean, I want to first review some high school math. Remember equations?</p>

          <div class="equation">y = x</div>
          <figure><img src="../imgs/techblog/linear.png" /></figure>
          <div class="equation">y = 1</div>
          <figure><img src="../imgs/techblog/constant.png" /></figure>
          <div class="equation">y = x^2</div>
          <figure><img src="../imgs/techblog/quadratic.png" /></figure>
          <div class="equation">y = log(x)</div>
          <figure><img src="../imgs/techblog/logarithmic.png" /></figure>

          <p>Remember what they mean?</p>
          <p>Basically, as a quick refresher, the line in the graph reflects change of y as x increases in value from left to right. So in the y = x graph, you’ll see that there’s a straight line, because y increases at the same rate that x does.</p>

          <p>Now, I want you to look back at the Big O connotations I use, and look at them the same way. In fact, someone has put together a chart with various Big-O graphs, and look, you’ll see it’s no different from the charts I drew above.</p>
          <figure><img src="../imgs/techblog/big-o.png" /><figcaption>(Credits to <a href="http://www.bigocheatsheet.com">bigocheatsheet.com</a>)</figcaption></figure>


          <p>Now, that you know what they look like, we can start talking about what the values mean.</p>

          <p>The O in Big-O stands for “order”, as in “Orders of Magnitude”. The x-axis measures the number of users/arguments used in a function, while the y-axis measures the computational intensity. We’re asking the question “how does computing power increase as we increase the number of arguments for this algorithm?"</p>

          <p>What Big-O is, is essentially an indicator of how time-intensive an algorithm is (basically any method/function within a program) as it scales. Algorithms take parameters, and sometimes a lot of them, and Big-O is the ultimate calculation that tells you how intensive this algorithm will become as we increase the number of arguments by large orders of magnitude (so we’re not comparing incrementally between 1 and 2, but rather, 1 to 10, and then 10 to 100, and then 100 to 1000, etc...)</p>

          <p>Let’s consider an example:</p>

          <div class="code">def linear(array)</div>
          <div class="code">  array.each do |x|</div>
          <div class="code">    puts x</div>
          <div class="code">  end</div>
          <div class="code">end</div>

          <div class="code">linear([1]) # => “1”</div>
          <div class="code">linear([1,2]) # => “1” “2”</div>
          <div class="code">linear([1,2,3]) # => “1” “2” “3”</div>

          <p>Notice if we only include one element in the array, then this function will run one calculation. If we have two, it will run two lines of calculation. This function’s complexity(this is actual terminology) is hence O(n) (much like the line y=x), because it’s a linear relationship between the number of elements and the time it takes to run.</p>

          <p>Now let’s look at a different example:</p>

          <div class="code">def quadratic(array)</div>
          <div class="code">  array.each do |x|</div>
          <div class="code">    array.each do|y|</div>
          <div class="code">      puts "#{x}, #{y}"</div>
          <div class="code">    end</div>
          <div class="code">  end</div>
          <div class="code">end</div>
          <div class="code">quadratic([1]) # => “1,1” (1 calculation)</div>
          <div class="code"></div>
          <div class="code">quadratic([1,2]) # => “1,1” “1,2” “2,1” “2,2” (4 calculation)</div>
          <div class="code">quadratic([1,2,3]) # => “1,1” “1,2” “1,3” “2,1” “2,2” “2,3” “3,1” “3,2” “3,3” (9 calculation)</div>

          <p>In this code you’ll find that the calculation effort is scaled exponentially with more arguments. When we have 3 parameters the code will take 9 times as long, and with 4 parameters, 16 times as long. Not very ideal, but it can happen. In this case, the complexity of this code id O(n^2). Get the gist?</p>

          <p>O(1) will always be the most favored condition, because it runs consistently regardless of the number of inputs.</p>

          <p>Here’s something I am copying from a stackoverflow answer, since there shouldn’t be a need to reinvent the wheel:</p>

          <figure class="quote">O(n^2): known as Quadratic complexity

          <ul>
            <li>1 item: 1 second</li>
            <li>10 items: 100 seconds</li>
            <li>100 items: 10000 seconds</li>
          </ul>

          Notice that the number of items increases by a factor of 10, but the time increases by a factor of 102. Basically, n=10 and so O(n^2) gives us the scaling factor n2 which is 102.
          O(n): known as Linear complexity

          <ul>
            <li>1 item: 1 second</li>
            <li>10 items: 10 seconds</li>
            <li>100 items: 100 seconds</li>
          </ul>

          This time the number of items increases by a factor of 10, and so does the time. n=10 and so O(n)'s scaling factor is 10.
          O(1): known as Constant complexity

          <ul>
            <li>1 item: 1 second</li>
            <li>10 items: 1 second</li>
            <li>100 items: 1 second</li>
          </ul>

          The number of items is still increasing by a factor of 10, but the scaling factor of O(1) is always 1.
          O(log n): known as Logarithmic complexity

          <ul>
            <li>1 item: 1 second</li>
            <li>10 items: 2 seconds</li>
            <li>100 items: 3 seconds</li>
            <li>1000 items: 4 seconds</li>
            <li>10000 items: 5 seconds</li>
          </ul>
          <figcaption><a href="http://stackoverflow.com/a/487300">- Credits to Ray Hidayat</a></figcaption></figure>


          <p>So what’s the relevance of all this complicated, abstract science? Well when it comes to coding, a lot of times a code can work fine and dandy during test/beta stages when you’re working with small sample sizes, whether its the number of users or sets of data points. However, I think most programmers code with the intent of eventually having thousands or millions of users. In this case, when we’re talking about O(n2) or more intensive complexities, programs and servers can collapse when datasets increase by orders of magnitude. A 10-fold increase in the numbers of users could result in 100 fold increase in computer power, so if these types of scaling issues aren’t carefully accounted for, a lot of issues could arise.</p>

          <p>Big-O can get very complicated, as programmers try to optimize their code for best efficiency, while maintaining readability of the code, and accounting for how big the datasets are going to get. For example, there are cases where codes are less efficient when there are a few arguments, but as n gets larger, the benefits become more obvious. That’s when it’s important to conduct a cost-benefit analysis, to determine which code will work best in the long run by accounting for long term goals.</p>

          <p>This is just the tip of the iceberg, but I recommend you check out the following links for some great explanations.</p>

          <p>Good luck in your learning, and happy coding!</p>

          <p>Further reading:</p>
          <ul>
          <li><a href="http://stackoverflow.com/questions/487258/plain-english-explanation-of-big-o">http://stackoverflow.com/questions/487258/plain-english-explanation-of-big-o</a></li>
          <li><a href="https://justin.abrah.ms/computer-science/big-o-notation-explained.html">https://justin.abrah.ms/computer-science/big-o-notation-explained.html</a></li>
          <li><a href="http://bigocheatsheet.com/">http://bigocheatsheet.com/</a></li>

      </article>
    </div>
    <footer>
      <div class="horizontal-line"></div>
      <p>&copy;2015 by Stranger Danger</p>
    </footer>
  </body>
</html>




